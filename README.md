
# План по размещению и воспроизводимому использованию моделей

Ниже описан целостный подход к доставке оффлайн-движков распознавания речи (Whisper и GigaAM) в составе сервиса, контролю артефактов и воспроизводимости окружений.

## 1. Цели и ограничения
- **Полностью оффлайн:** сервис не должен ходить в интернет после первоначальной подготовки окружения.
- **Детерминированность:** одинаковые входные данные → идентичный результат, независимо от машины.
- **Управление артефактами:** отслеживание, кто и какую версию модели использует, возможность отката.
- **Масштабируемость:** поддержка нескольких движков и наборов весов в едином пайплайне доставки.

## 2. Каталог артефактов
```
offline_asr_service/
├─ models/
│  ├─ manifest.json
│  ├─ whisper/
│  │  ├─ base.en/
│  │  │  ├─ model.bin
│  │  │  └─ checksum.txt
│  │  └─ large-v3/
│  └─ gigaam/
│     ├─ gigaam-base/
│     └─ gigaam-pro/
└─ app/
```
- `manifest.json` описывает каждую модель (uid, тип, версия, лицензия, sha256), что облегчает валидацию и автоматическую загрузку.
- Каждая папка конкретной модели содержит веса, вспомогательные файлы (`tokenizer.json`, `config.yaml`) и контрольную сумму. Путь кодируется в `schemas.ModelMetadata`.
- Для экономии места бинарники хранятся вне Git. Возможные варианты: `git lfs`, `dvc`, артефактный менеджер (S3/MinIO/Artifactory). Главное — хранить `manifest.json` и контрольные суммы в Git.

## 3. Подготовка моделей
1. **Загрузка и конвертация**: отдельный скрипт (`scripts/prepare_models.py`) скачивает Whisper / GigaAM, приводит к унифицированному формату (например, PyTorch `.bin` или ONNX), рассчитывает `sha256`.
2. **Фиксация параметров**: скрипт сохраняет точную версию исходных репозиториев (commit hash) и зависимости (версия `transformers`, `sentencepiece` и т.д.).
3. **Регистрация в манифесте**: добавление записи с полями:
   ```json
   {
     "id": "whisper-large-v3",
     "engine": "whisper",
     "precision": "fp16",
     "source": {
       "repo": "https://github.com/openai/whisper",
       "commit": "abcd123"
     },
     "artifacts": [
       {"path": "models/whisper/large-v3/model.bin", "sha256": "..."}
     ]
   }
   ```
4. **Проверка**: прогон короткого smoke-теста (`app/tests/test_inference.py`) с фиксированным аудио и эталонной транскрипцией.

## 4. Использование моделей в рантайме
- В `app/config.py` задаётся `MODEL_REGISTRY_PATH` и активные пресеты (например, `DEFAULT_WHISPER_MODEL=whisper-large-v3`).
- `app/device.py` проверяет наличие весов локально, сравнивает checksum, иначе — подсказывает, откуда взять архив (монтаж USB, корпоративный артефакт-сервер).
- `app/asr/base.py` реализует интерфейс `TranscriptionEngine` с методами `load(model_id)` и `transcribe(audio_chunk)`. Конкретные реализации (`whisper_engine.py`, `gigaam_engine.py`) читают конфиг из манифеста, что исключает «магические» пути.
- Логи содержат `model_id` и `manifest_version`, чтобы аудит мог отследить, какой движок использовался.

## 5. Контроль артефактов
- **Версионирование:** каждая правка `manifest.json` сопровождается bump версии (например, `manifest_version: 3`) и описанием изменений в CHANGELOG.
- **Контроль целостности:** перед деплоем CICD пересчитывает контрольные суммы и сравнивает с манифестом.
- **Хранение архивов:** для переносов оффлайн-пакетов используется зашифрованный tar (`models_bundle_v3.tar.zst + sha256`). Скрипт `scripts/package_models.sh` собирает архив строго из `manifest.json`, исключая неописанные файлы.

## 6. Воспроизводимость окружений
- **Зависимости Python:** источник истины — `requirements.lock` (сгенерирован через `pip-tools` или `uv pip compile`) с фиксированными версиями. В `requirements.txt` оставляем high-level группы и инструкцию «не редактировать вручную».
- **Системные зависимости:** документировать версии CUDA/cuDNN, FFmpeg и др. в `README` + скрипт `scripts/check_env.py`, который валидирует установленное окружение.
- **CI-пайплайн:** job `env-check` собирает Docker-образ с конкретным базовым образом (например, `nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04`), устанавливает зависимости из lock-файла и прогоняет тесты. Хэш образа сохраняется рядом с `manifest.json`.
- **Раздача окружения оффлайн:** подготовить `conda-pack` или `virtualenv` архивы, версионируемые аналогично моделям (`envs/python310_cuda12.tar.gz + sha256`).

## 7. Рекомендованный рабочий процесс
1. **Выбор модели** → запуск `scripts/prepare_models.py --engine whisper --variant large-v3`.
2. **Модернизация манифеста** → добавление записи + фиксация через PR.
3. **Сборка оффлайн-пакета** → `make package-offline` создаёт `offline_bundle_vX.zip` (веса, lock-файлы, скрипты развертывания).
4. **Доставка** → проверка checksum, распаковка в `models/` и `envs/`.
5. **Запуск** → `APP_MODEL_ID=whisper-large-v3 python app/main.py --manifest models/manifest.json`.

Следуя этой схеме, можно уверенно распространять и обновлять Whisper/GigaAM оффлайн, сохраняя контроль версий и гарантированную воспроизводимость.
